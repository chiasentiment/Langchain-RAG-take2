{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f48a8e-a6fb-4baf-9ff2-3d5bb82b23cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Build a RAG agent that can run on llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1ac3d2-5e5b-404e-b10a-2b8721e0097a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "![Llama RAG implementation.png](./Llama RAG implementation.png \"Llama RAG implementation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a996e05b-4730-4ac5-9690-db91bec855b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from databricks_langchain import ChatDatabricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797142e9-70a4-42cf-a37d-e358c0ec3194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from  dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e160c1c-80ea-48a7-8b32-668f26930c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(\n",
    "    endpoint='otc-lama-poc',\n",
    "    temperature=0,\n",
    "    max_tokens=250\n",
    ")\n",
    "chat_model_json = ChatDatabricks(\n",
    "    endpoint='otc-lama-poc',\n",
    "    temperature=0,\n",
    "    max_tokens=250,\n",
    "    return_json=True,\n",
    "    # json_format=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753c05ab-03cd-40ac-a5eb-cf39a7feb20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56fbb6f-d8aa-4595-8587-eaf1a28c395d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01abc69a-5950-44ed-aaa2-8e3c85c9bb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load documents\n",
    "documents = [WebBaseLoader(url).load() for url in urls]\n",
    "doc_list = [item for sublist in documents for item in sublist] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f090430-2c04-4d45-85e6-7a4dc9353b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "doc_splits = text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc91d1e-942a-4404-bcfc-4bffc602be17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load vector store\n",
    "vectorstore = SKLearnVectorStore.from_documents(documents=doc_splits, \n",
    "                                                embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05219561-e184-446a-bd99-aadf6c919ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ROUTER \n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "#Prompt\n",
    "router_instructions = '''You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "Return JSON format with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question. \n",
    "No formatting or comments required. Pure json format.'''\n",
    "\n",
    "#Test\n",
    "test_websearch = chat_model_json.invoke([SystemMessage(content=router_instructions), \n",
    "                                         HumanMessage(content=\"who won champions tropy 2025?\")])\n",
    "test_vectorstore = chat_model_json.invoke([SystemMessage(content=router_instructions), HumanMessage(content=\"What is prompt engineering?\")])\n",
    "print (json.loads(test_websearch.content))\n",
    "print (json.loads(test_vectorstore.content))                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62eb9797-48c3-41f7-ab58-de4dffc510c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RETRIEVE DOCUMENTS\n",
    "retriever = vectorstore.as_retriever(k=3)\n",
    "retriever.invoke(\"Agent memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47685112-b5bc-4cd2-9e74-8ff26ba35965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GRADE DOCUMENTS\n",
    "#Doc Grade Instructions\n",
    "doc_grade_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "#Doc Grade Prompt\n",
    "doc_grade_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return only JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question. No formatting or comments required. Pure json format.\"\"\"\n",
    "\n",
    "question = \"What is chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grade_prompt_formatted = doc_grade_prompt.format(document=doc_txt, question=question)\n",
    "\n",
    "results = chat_model_json.invoke([SystemMessage(content=doc_grade_instructions), HumanMessage(content=doc_grade_prompt_formatted)])\n",
    "print(json.loads(results.content))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1d3676-8fbf-4216-9b36-133dd8697d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GENERATE ANSWER\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "#post processing\n",
    "def format_docs (docs):\n",
    "    return  \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#test\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = format_docs(docs)\n",
    "rag_prompt_formated = rag_prompt.format(context=doc_txt, question=question)\n",
    "generation = chat_model.invoke([HumanMessage(content=rag_prompt_formated)])\n",
    "print (generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbc65fc-0452-4e74-98a0-a9cb953cbdb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HALLUCINATION CHECKER\n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "#Grader prompt\n",
    "hallucination_grader_prompt = \"\"\" FACTS: \\n\\n {docs} \\n\\n STUDENT ANSWER: \\n\\n {generation} \\n\\n Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\n",
    "Only JSON formated. No formatting or comments required. no ``` in the start or end\"\"\"\n",
    "\n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(docs=doc_txt, generation=generation.content)\n",
    "result = chat_model_json.invoke([SystemMessage(content=hallucination_grader_instructions), HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588fdded-a8db-4473-a649-b74c5a0eb12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GRADE THE ANSWER\n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "answer_grader_prompt = \"\"\"QUESTION:\\n\\n {question} \\n\\n STUDENT ANSWER: {generation}.\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\n",
    "only json formated. No formatting or comments required.\"\"\"\n",
    "\n",
    "#Test\n",
    "question = \"What are the vision models released today as part of Llama 3.2?\"\n",
    "answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models.\"\n",
    "\n",
    "answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=answer)\n",
    "result = chat_model_json.invoke([SystemMessage(content=answer_grader_instructions), HumanMessage(content=answer_grader_prompt_formatted)])\n",
    "\n",
    "json.loads(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5f94d1-547b-4ff8-b250-e89ab850dba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Websearch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5903e147-44c0-4d82-ac35-de6239b4fc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc15499-e391-4170-962a-69c0f6106fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15633f2-b6b0-4036-b92e-f5fbe7fb1335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "#Langgragh agent state class\n",
    "class AgentState(TypedDict):\n",
    "    '''\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    '''\n",
    "    question: str  #User Question\n",
    "    generation: str #LLM Generation\n",
    "    web_search: str # Binary decision to run websearch\n",
    "    max_retries: int # Max number of retries for answer generation\n",
    "    answers: int # number of answers generated\n",
    "    loop_step: [int, operator.add] # \n",
    "    documents: List[str] # list of retrieved documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4669478e-0549-4ef5-ae0b-76e77a59601d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6c85b2-bd56-4b0f-a70d-696cf5428f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve Node\n",
    "def retrieve(state: AgentState):\n",
    "    '''\n",
    "    Retrieves documents from vector store\n",
    "\n",
    "    Args:\n",
    "    state (dict): The current graph state\n",
    "    Returns:\n",
    "    state(dict): New key added to state, documents that contain retrieved documents\n",
    "    '''\n",
    "    print (\"---RETRIEVE---\")\n",
    "    question = state['question']\n",
    "    # Write retrieved documents to documents key in state\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bfa0cd-4996-4b24-9005-f2cb659638bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    def generate(state: AgentState):\n",
    "        '''\n",
    "        Generate answer using RAG on retrieved documents\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "        Returns:\n",
    "            state(dict): New key added to state, generation from LLM\n",
    "        '''\n",
    "        print (\"entering generate function\")\n",
    "        print (\"---GENERATE---\")\n",
    "        question = state['question']\n",
    "        # print (f'question: {question}')\n",
    "        documents = state['documents']\n",
    "        # print (f'documents: {documents}')\n",
    "\n",
    "        loop_step = state.get('loop_step', 0)\n",
    "        # print (f\"loop_step: {loop_step}\")\n",
    "\n",
    "        #RAG Generation:\n",
    "        docs_txt = format_docs(documents)\n",
    "        # print (f\"docs_text: {docs_txt}\")\n",
    "        rag_prompt_formated = rag_prompt.format(context=docs_txt, question=question)\n",
    "        generation = chat_model.invoke([HumanMessage(content=rag_prompt_formated)])\n",
    "        return {\"generation\": generation, \"loop_step\": loop_step+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a246e1-df32-4531-9422-70459a5e26aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def grade_documents (state: AgentState):\n",
    "    '''\n",
    "    Determines where the retrieved documents are relevant to the question.\n",
    "    If any document is not relevant, we will set a flag to run websearch.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state(dict): Filtered out irrelevant documents and updated web_search state if there is no relevant documents found\n",
    "    '''\n",
    "    print (\"----CHECK DOCUMENTS RELEVANCE TO QUESTION----\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        doc_grade_prompt_formatted = doc_grade_prompt.format(document=d.page_content, question=question)\n",
    "        result = chat_model.invoke([SystemMessage(content= doc_grade_instructions),\n",
    "                                    HumanMessage(content=doc_grade_prompt_formatted)])\n",
    "        print (result.content)\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        #Document Relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print (\"---- GRADER: DOCUMENT RELEVANT----\")\n",
    "            filtered_docs.append(d)\n",
    "        #Documents not relevant\n",
    "        else:\n",
    "            print (\"---- GRADER: DOCUMENT NOT RELEVANT----\") \n",
    "            #we do not include documents in the filtered_docs\n",
    "            # web_search=\"Yes\"\n",
    "            continue\n",
    "        #If there are no relevant documents then direct to websearch\n",
    "    if len(filtered_docs) == 0:\n",
    "        web_search=\"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2409d1ee-634c-4f4d-a36d-09562388ac61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def web_search(state: AgentState):\n",
    "    \"\"\"\n",
    "    Websearch is based on the question\n",
    "\n",
    "    Args:\n",
    "    state(dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "    state(dict): Append web results to documents\n",
    "    \"\"\"\n",
    "    print (\"---WEB SEARCH---\")\n",
    "    question = state['question']\n",
    "    documents = state.get('documents', [])\n",
    "    print (f'question: {question}')\n",
    "    print (f'documents: {documents}')\n",
    "    #web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    print (f'web search results docs: {docs}')\n",
    "    web_results = \"\\n\".join(d['content'] for d in docs)\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d858727d-53bf-4052-ba5e-9951f4486ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#EDGES\n",
    "def route_question(state: AgentState):\n",
    "    \"\"\"\n",
    "    Route question to web serch or RAG\n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "    returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    print (\"---ROUTE QUESTION---\")\n",
    "    route_question = chat_model_json.invoke([SystemMessage(content=router_instructions),HumanMessage(content=state['question'])])\n",
    "    source = json.loads(route_question.content)[\"datasource\"]  \n",
    "    if source == \"websearch\":\n",
    "        print (\"----ROUTE QUESTION: WEB SEARCH----\")\n",
    "        return \"websearch\"\n",
    "    elif source == \"vectorstore\":\n",
    "        print (\"----ROUTE QUESTION: RAG----\")\n",
    "        return \"vectorstore\"\n",
    "    else:\n",
    "        print (\"Option invalid: ERROR in flow\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d7b481-1aa3-41c3-b062-9063743130d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def decide_to_generate(state: AgentState):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call   \n",
    "    \"\"\"\n",
    "    print (\"----ASSES GRADED DOCUMENTS----\")\n",
    "    question = state['question']\n",
    "    web_search=state['web_search']\n",
    "    filtered_documents = state['documents']\n",
    "\n",
    "    if \"web_search\" == \"Yes\":\n",
    "        ## All documents have been filtered check relevance\n",
    "        #We will re-generate a new query\n",
    "        print(\"----DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEBSEARCH ---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        #We have relevant documents, so generate answer\n",
    "        print (\"----DECISION: GENERATE----\")\n",
    "        return \"generate\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b1b26a-2d92-4b19-9713-5d1ae61f0358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state: AgentState):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call   \n",
    "    \"\"\"\n",
    "    print (\"----CHECK HALLUCINATION----\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    generation = state['generation']\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format( docs=format_docs(documents), generation=generation.content)\n",
    "    result = chat_model_json.invoke([SystemMessage(content=hallucination_grader_instructions), HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "    # print (f'hallucination grader instructions {hallucination_grader_instructions}')\n",
    "    # print (f'hallucination grader prompt: {hallucination_grader_prompt_formatted}')\n",
    "    # print (f'{result.content}')\n",
    "    # print (f'hallucination results: {json.loads(result.content)}')\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "    #Check hallucination\n",
    "    if grade.lower() == \"yes\":\n",
    "        print (\"----DECISION: GENERATION GROUNDED IN DOCUMENTS ----\")\n",
    "        #check question answering\n",
    "        print (\"----CHECK GENERATION vs QUESTION----\")\n",
    "        #Test using question and generation from above\n",
    "        answer_grader_prompt_formatted  = answer_grader_prompt.format(question=question, generation=generation.content)\n",
    "        result = chat_model_json.invoke([SystemMessage(content=answer_grader_instructions), HumanMessage(content=answer_grader_prompt_formatted)])\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        if grade.lower() == \"yes\":\n",
    "            print (\"----DECISION: GENERATION GROUNDED IN DOCUMENTS AND QUESTION ----\")  \n",
    "            return \"useful\"\n",
    "        elif state['loop_step'] <= max_retries:\n",
    "            print (\"----DECISION: GENERATION GROUNDED IN DOCUMENTS BUT NOT QUESTION, RETRYING ----\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print (\"----DECISION: GENERATION GROUNDED IN DOCUMENTS BUT NOT QUESTION, RETRY LIMIT REACHED ----\")\n",
    "            return \"max retries\"\n",
    "    elif state['loop_step'] <= max_retries:\n",
    "        print (\"----DECISION: GENERATION NOT GROUNDED IN DOCUMENTS, RETRYING ----\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print (\"----DECISION: GENERATION NOT GROUNDED IN DOCUMENTS, RETRY LIMIT REACHED ----\")\n",
    "        return \"max retries\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea55b929-93f6-4af1-8174-263d8531f325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a11973-5321-413f-90c3-b9352a4ecc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c7cfa3-bea3-4712-827a-7a803a028f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "#Define the nodes\n",
    "graph.add_node(\"websearch\", web_search) #websearch\n",
    "graph.add_node(\"generate\", generate) # generate\n",
    "graph.add_node(\"retrieve\", retrieve) #retrieve documents\n",
    "graph.add_node(\"grade_documents\", grade_documents) #grade documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002e37cb-e41e-470c-b924-72e012753baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#build graph\n",
    "graph.set_conditional_entry_point(\n",
    "  route_question,\n",
    "  {\n",
    "    \"websearch\": \"websearch\",\n",
    "    \"vectorstore\": \"retrieve\"\n",
    "  }\n",
    ")\n",
    "graph.add_edge(\"websearch\", \"generate\")\n",
    "graph.add_edge(\"retrieve\", \"grade_documents\")\n",
    "graph.add_conditional_edges(\"grade_documents\",\n",
    "                            decide_to_generate,\n",
    "                            {\n",
    "                              \"websearch\": \"websearch\",\n",
    "                              \"generate\": \"generate\"\n",
    "                            })\n",
    "graph.add_conditional_edges(\"generate\",\n",
    "                            grade_generation_v_documents_and_question,\n",
    "                            {\n",
    "                              \"not supported\": \"generate\",\n",
    "                              \"useful\": END,\n",
    "                              \"not useful\": \"websearch\",\n",
    "                              \"max retries\": END\n",
    "                            })\n",
    "compiled_graph = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2111ae18-8b15-4d28-9c2c-2688edce8625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd6ae5a-9499-4f5a-a7b4-ff8b46909ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"Adversarial Attacks on LLMs\",\"max_retries\":3}\n",
    "for event in compiled_graph.stream(inputs, stream_mode=\"values\"):\n",
    "    if \"question\" in event:\n",
    "        print (f'Question: {event[\"question\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911f15f8-47fe-457a-a827-3072457ba0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846de738-a647-4676-a840-fa1a650d9645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd658a3f-ac8e-4f64-93e1-e2c35d2863ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Langchain-langgraph-RAG-modified",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
